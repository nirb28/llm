{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirb28/llm/blob/main/distillation/accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72bb998-334e-4021-b5df-cf59d98dce5e",
      "metadata": {
        "id": "a72bb998-334e-4021-b5df-cf59d98dce5e"
      },
      "source": [
        "### Calculate a Target Accuracy\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} \\times100\n",
        "$$\n",
        "\n",
        "Calculating a base accuracy of Llama 3.1 405B vs the expectation from the dataset's labels gives us a target accuracy to aim for when training our student language model"
      ]
    },
    {
      "cell_type": "code",
      "id": "d58be01e-e7b5-450a-9e44-cd5765ad73e9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-10T04:07:58.750434Z",
          "start_time": "2024-08-10T04:07:58.713431Z"
        },
        "id": "d58be01e-e7b5-450a-9e44-cd5765ad73e9",
        "outputId": "0eec3a9c-3040-4b81-8b93-474889c4abce"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your updated CSV file\n",
        "df = pd.read_csv(str(get_project_root())+\"/data/tweet_sentiment/test.csv\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(df['label_text'], df['Llama_405B_label_text'])\n",
        "\n",
        "print(f\"Accuracy of Llama 3.1 405B: {accuracy:.2%}\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Llama 3.1 405B: 63.66%\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "09cc7fae-19a6-43cc-9672-bdc5d25781b4",
      "metadata": {
        "id": "09cc7fae-19a6-43cc-9672-bdc5d25781b4"
      },
      "source": [
        "---\n",
        "# Training Student Model\n",
        "\n",
        "Our student model will be [FacebookAI/roberta-base](https://huggingface.co/FacebookAI/roberta-base), a 125 million parameter language model. We'll be fine tuning this model on our Llama 3.1 405B annotated data using [AutoTrain Adance](https://github.com/huggingface/autotrain-advanced), HuggingFaces' packaged opensource solution for lowcode model training. They make it as easy as possible to run on local hardware, or via GPU accelerator platforms like Google's Colab or [HuggingFace Spaces](https://huggingface.co/autotrain).\n",
        "\n",
        "We'll be using the *train* segment of the annotations generated, specifically the text from the original tweets and label as the Llama 3.1 405B generated label, passing in these hyperparameters (AutoTrain format)\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"auto_find_batch_size\": \"false\",\n",
        "  \"eval_strategy\": \"epoch\",\n",
        "  \"mixed_precision\": \"fp16\",\n",
        "  \"optimizer\": \"adamw_torch\",\n",
        "  \"scheduler\": \"linear\",\n",
        "  \"batch_size\": \"16\",\n",
        "  \"early_stopping_patience\": \"5\",\n",
        "  \"early_stopping_threshold\": \"0.01\",\n",
        "  \"epochs\": \"5\",\n",
        "  \"gradient_accumulation\": \"1\",\n",
        "  \"lr\": \"0.00005\",\n",
        "  \"logging_steps\": \"-1\",\n",
        "  \"max_grad_norm\": \"1\",\n",
        "  \"max_seq_length\": \"128\",\n",
        "  \"save_total_limit\": \"1\",\n",
        "  \"seed\": \"42\",\n",
        "  \"warmup_ratio\": \"0.1\",\n",
        "  \"weight_decay\": \"0\"\n",
        "}\n",
        "```\n",
        "\n",
        "Final trained model published at [AdamLucek/roberta-llama3.1405B-twitter-sentiment](https://huggingface.co/AdamLucek/roberta-llama3.1405B-twitter-sentiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2865a04-9705-4445-9bb0-2dc111d51ffd",
      "metadata": {
        "id": "f2865a04-9705-4445-9bb0-2dc111d51ffd"
      },
      "source": [
        "---\n",
        "# Testing Out the Fine Tuned Model\n",
        "\n",
        "We'll be using [HuggingFace's Transformers Package Pipelines](https://huggingface.co/docs/transformers/en/main_classes/pipelines) to easily load and run inference using our trained model"
      ]
    },
    {
      "cell_type": "code",
      "id": "02d0f3cb-e68c-4714-a3a9-0b3d4785ddd5",
      "metadata": {
        "id": "02d0f3cb-e68c-4714-a3a9-0b3d4785ddd5"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create sentiment Analysis pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"AdamLucek/roberta-llama3.1405B-twitter-sentiment\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "b98a86f1-e322-4abf-9eab-1cfefe925dcb",
      "metadata": {
        "id": "b98a86f1-e322-4abf-9eab-1cfefe925dcb"
      },
      "source": [
        "classifier(\"Want to get a Blackberry but can`t afford it. Just watching the telly and relaxing. Hard sesion tomorrow.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "e38c6ee8-9ec3-488b-b0a1-5491b0d18beb",
      "metadata": {
        "id": "e38c6ee8-9ec3-488b-b0a1-5491b0d18beb"
      },
      "source": [
        "---\n",
        "# Visualizing Accuracy\n",
        "\n",
        "Now to calculate and compare our fine tuned model's accuracy to compare to LLama 3.1 405B. For fun, also ran tests using [GPT-4o-Mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) and a [generalized multilingual sentiment model of similar size](https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student)."
      ]
    },
    {
      "cell_type": "code",
      "id": "24aaa1c1-f499-4486-8583-a0e0c880d082",
      "metadata": {
        "id": "24aaa1c1-f499-4486-8583-a0e0c880d082"
      },
      "source": [
        "# Calculate accuracy\n",
        "result_df = pd.read_csv(\"/Users/alucek/Documents/Jupyter_Notebooks/synthetic_data/tweet_sentiment/total_tests.csv\")\n",
        "\n",
        "llama31_405B_accuracy = accuracy_score(result_df['label_text'], result_df['Llama_405B_label_text'])\n",
        "gpt4omini_accuracy = accuracy_score(result_df['label_text'], result_df['GPT4o_mini_label'])\n",
        "ft_roberta = accuracy_score(result_df['label_text'], result_df['Roberta_FT'])\n",
        "multilingual = accuracy_score(result_df['label_text'], result_df['ML_Roberta'])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "e65df45a-a658-4cb6-a157-5d7656915f75",
      "metadata": {
        "id": "e65df45a-a658-4cb6-a157-5d7656915f75"
      },
      "source": [
        "### Accuracy Graph"
      ]
    },
    {
      "cell_type": "code",
      "id": "ffaa51ca-f77d-49ba-9dbd-740f8eba9570",
      "metadata": {
        "id": "ffaa51ca-f77d-49ba-9dbd-740f8eba9570"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "models = ['LLama 3.1 405B', 'GPT-4o-Mini', 'Multilingual-Roberta', 'Fine Tuned Roberta']\n",
        "accuracies = [llama31_405B_accuracy, gpt4omini_accuracy, multilingual, ft_roberta]\n",
        "\n",
        "# Convert accuracies to percentages\n",
        "percentages = [acc * 100 for acc in accuracies]\n",
        "\n",
        "# Combine models and percentages into a list of tuples, sort by accuracy in descending order\n",
        "sorted_data = sorted(zip(models, percentages), key=lambda x: x[1], reverse=True)\n",
        "sorted_models, sorted_percentages = zip(*sorted_data)\n",
        "\n",
        "# Create bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(sorted_models, sorted_percentages, color=['#cde8f4', '#d6f5bf', '#fdd1d1', '#e3d9f0'], edgecolor='black', linewidth=1)\n",
        "plt.title(\"Accuracy of Models on Twitter Sentiment Classification\", fontsize=16)\n",
        "plt.xlabel(\"Model\", fontsize=12)\n",
        "plt.ylabel(\"Accuracy (%)\", fontsize=12)\n",
        "plt.ylim(0, 100)  # Set y-axis range from 0 to 100%\n",
        "\n",
        "# Add value labels on top of each bar\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, height,\n",
        "             f'{height:.2f}%',\n",
        "             ha='center', va='bottom')\n",
        "\n",
        "# Display\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "066be671-731b-45ee-bd4f-62fe5c6958a9",
      "metadata": {
        "id": "066be671-731b-45ee-bd4f-62fe5c6958a9"
      },
      "source": [
        "Roughly the same performance at **0.03%** the size!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "239fe7eb-76c1-4239-8efc-d284bc32b281",
      "metadata": {
        "id": "239fe7eb-76c1-4239-8efc-d284bc32b281"
      },
      "source": [
        "### Count of Sentiments\n",
        "\n",
        "Also interesting to consider the distribution of sentiment labels across these models"
      ]
    },
    {
      "cell_type": "code",
      "id": "b67a0124-1b19-4639-9d88-ec13bb188e31",
      "metadata": {
        "id": "b67a0124-1b19-4639-9d88-ec13bb188e31"
      },
      "source": [
        "# List of models and their corresponding label columns\n",
        "models = {\n",
        "    'Llama 3.1 405B': 'Llama_405B_label_text',\n",
        "    'Roberta FineTune': 'Roberta_FT',\n",
        "    'Multilingual Distilbert': 'ML_Roberta',\n",
        "    'GPT 4o Mini': 'GPT4o_mini_label'\n",
        "}\n",
        "\n",
        "# Calculating counts\n",
        "label_counts = {model: {'positive': 0, 'neutral': 0, 'negative': 0} for model in models.keys()}\n",
        "for model, column in models.items():\n",
        "    label_counts[model]['positive'] = result_df[result_df[column] == 'positive'].shape[0]\n",
        "    label_counts[model]['neutral'] = result_df[result_df[column] == 'neutral'].shape[0]\n",
        "    label_counts[model]['negative'] = result_df[result_df[column] == 'negative'].shape[0]\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "counts_df = pd.DataFrame(label_counts).T\n",
        "\n",
        "# Plotting with outlines\n",
        "ax = counts_df.plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    figsize=(10, 7),\n",
        "    color=['#b3e8d1', '#fccbb2', '#c7d4eb'],\n",
        "    edgecolor='black'\n",
        ")\n",
        "\n",
        "# Add counts on the bars\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "    ax.text(x + width/2,\n",
        "            y + height/2,\n",
        "            f'{int(height)}',\n",
        "            ha='center',\n",
        "            va='center')\n",
        "\n",
        "plt.title('Label Counts per Model')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Label')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "691afcdf-45b4-4644-aa23-36ac591b2244",
      "metadata": {
        "id": "691afcdf-45b4-4644-aa23-36ac591b2244"
      },
      "source": [
        "---\n",
        "# Addtional Notes:\n",
        "\n",
        "\n",
        "Consider the costs:\n",
        "* ~\\$0.08 for gpt4o via OpenAI\n",
        "* ~$1.80 for Llama 3.1 405B via Fireworks\n",
        "\n",
        "And consider the time cost too! Much slower inference with the foundational models compared to the distilled language model.\n",
        "\n",
        "Note that we are not aiming for higher accuracy here, rather a standard metric to assess our fine tuned model to see if it performs now as well as the foundation model\n",
        "\n",
        "And a big shoutout to Moritz Laurer for the https://huggingface.co/blog/synthetic-data-save-costs blog, much of which the primary methodology that guided this notebook was heavily inspired by."
      ]
    },
    {
      "cell_type": "code",
      "id": "2d1e4432-4024-4638-bd95-73ec4060b4db",
      "metadata": {
        "id": "2d1e4432-4024-4638-bd95-73ec4060b4db"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}