{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nirb28/llm/blob/main/distillation/data_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "b5c34e2ed474edb4"
  },
  {
   "cell_type": "markdown",
   "id": "5b71c406-39c3-4463-889d-51a8f06f7450",
   "metadata": {
    "id": "5b71c406-39c3-4463-889d-51a8f06f7450"
   },
   "source": [
    "# Language Model Distillation\n",
    "\n",
    "<img src=\"https://arxiv.org/html/2402.13116v3/x2.png\" width=600>\n",
    "\n",
    "Model Distillation is the process of using a large foundation model/high parameter LLMs to create annotated data for a specific task. That data is then used to fine tune a lightweight language model on the same task, allowing the smaller parameter model to perform as well as the foundation model at a fraction of the cost, energy consumption, and time.\n",
    "\n",
    "Per the paper [A Survey on Knowledge Distillation of Large Language Models](https://arxiv.org/pdf/2402.13116), \"This process is akin to transferring the ‘knowledge’ of a highly skilled teacher to a student, wherein the student (e.g., open-source LLM) learns to mimic the performance characteristics of the teacher (e.g., proprietary LLM).\"\n",
    "\n",
    "Most tasks that LLMs are applied to don't utilize the entire capability and power of a full size foundation model, so *why not distill down your one specific application into its own model?*\n",
    "\n",
    "**In this notebook we'll be:**\n",
    "1. Using [Llama 3.1 405B](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B) to classify the sentiment of tweets, and\n",
    "2. Use that dataset to train [Roberta-base](https://huggingface.co/FacebookAI/roberta-base) a 125 million parameter language model.\n",
    "\n",
    "We end up with a model that performs with the same accuracy, at 0.03% of the size!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae98b73-8a34-4d48-80ff-dbe9bc05f7e7",
   "metadata": {
    "id": "fae98b73-8a34-4d48-80ff-dbe9bc05f7e7"
   },
   "source": [
    "---\n",
    "# Teacher Model Data Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a002a4-19c9-4b7b-a3fe-aa8b0a6e7b33",
   "metadata": {
    "id": "54a002a4-19c9-4b7b-a3fe-aa8b0a6e7b33"
   },
   "source": [
    "Dataset consists of tweets with labeled sentiments, [mteb/tweet_sentiment_extraction](https://huggingface.co/datasets/mteb/tweet_sentiment_extraction). We will be using the tweet texts along with prompting to generate the \"knowledge\" annotation. This data is what becomes the training data for distillation.\n",
    "\n",
    "This would become simply fine-tuning/model training if the data is not generated by an LLM teacher! (i.e. human annotated data)\n",
    "\n",
    "For the sake of demonstration, assuming this data is not annoted already.\n",
    "\n",
    "### Importing Existing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "bcac2ecb-ff4c-4423-8304-0df8f2ccf43f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T00:01:15.820699Z",
     "start_time": "2024-08-10T00:01:09.071140Z"
    },
    "id": "bcac2ecb-ff4c-4423-8304-0df8f2ccf43f"
   },
   "source": [
    "# use hf datasets package to get the twitter sentiment data\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"mteb/tweet_sentiment_extraction\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7714b2cc-13bb-4ff5-a847-80b44880a926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T00:01:15.837575Z",
     "start_time": "2024-08-10T00:01:15.823711Z"
    },
    "id": "7714b2cc-13bb-4ff5-a847-80b44880a926",
    "outputId": "f11de79a-7752-4c5f-8c0b-4efefaf459e8"
   },
   "source": [
    "ds['train'][0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a45182de-9013-4905-a0d1-fbc82e25262e",
   "metadata": {
    "id": "a45182de-9013-4905-a0d1-fbc82e25262e"
   },
   "source": [
    "### Creating CSV\n",
    "\n",
    "Converting a subset to a local csv for easy manipulation and sampling.\n",
    "\n",
    "Train/split subset via HuggingFace dataset available [AdamLucek/twittersentiment-llama-3.1-405B-labels](https://huggingface.co/datasets/AdamLucek/twittersentiment-llama-3.1-405B-labels)"
   ]
  },
  {
   "cell_type": "code",
   "id": "49973fc8-f8ea-4bb8-ae8d-7abd8d244080",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T00:05:09.169529Z",
     "start_time": "2024-08-10T00:05:09.097544Z"
    },
    "id": "49973fc8-f8ea-4bb8-ae8d-7abd8d244080",
    "outputId": "1bddf9e8-30f1-4988-fa14-a73ce781032b"
   },
   "source": [
    "import csv\n",
    "import os\n",
    "from common.utils import get_project_root\n",
    "def convert_to_csv(ds, start_index, end_index, output_folder):\n",
    "\n",
    "    output_file = os.path.join(output_folder, f\"twittersentiment_{start_index}_{end_index}.csv\")\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['id', 'text', 'label', 'label_text']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for i in range(start_index, min(end_index, len(ds['train']))):\n",
    "            row = ds['train'][i]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"CSV file created: {output_file}\")\n",
    "\n",
    "# Usage\n",
    "start_index = 5001\n",
    "end_index = 6000\n",
    "output_folder = str(get_project_root())+\"/data/tweet_sentiment\"\n",
    "convert_to_csv(ds, start_index, end_index, output_folder)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "70126ca8-e623-42fb-aa19-e33e5bfbd7c9",
   "metadata": {
    "id": "70126ca8-e623-42fb-aa19-e33e5bfbd7c9"
   },
   "source": [
    "### Cleaning the CSV\n",
    "\n",
    "Simple script for dropping blank values that may break processing later on"
   ]
  },
  {
   "cell_type": "code",
   "id": "d126d914-1b31-4a08-a4c7-30e874f110c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T01:21:18.779651Z",
     "start_time": "2024-08-10T01:21:14.772642Z"
    },
    "id": "d126d914-1b31-4a08-a4c7-30e874f110c9",
    "outputId": "42ceb7f5-3cb1-4a44-ff41-06fcf63f3453"
   },
   "source": [
    "import pandas as pd\n",
    "from common.utils import get_project_root\n",
    "# Load the CSV file\n",
    "unclean_df = pd.read_csv(str(get_project_root())+'/data/tweet_sentiment/twittersentiment_5001_6000.csv')\n",
    "\n",
    "# Remove rows where the 'text' label is blank\n",
    "df_cleaned = unclean_df[unclean_df['text'].notna() & (unclean_df['text'].str.strip() != '')]\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_cleaned.to_csv(str(get_project_root())+\"/data/tweet_sentiment/twittersentiment_5001_6000_cleaned.csv\", index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a9e0d352-306c-46e3-b73b-30428f52dbb5",
   "metadata": {
    "id": "a9e0d352-306c-46e3-b73b-30428f52dbb5"
   },
   "source": [
    "---\n",
    "# Setting Up Teacher LLM\n",
    "\n",
    "[Llama 3.1 405B](https://ai.meta.com/blog/meta-llama-3-1/) explicitly states that *Our new model will enable the community to unlock new workflows, such as synthetic data generation and model distillation* in Meta's [release blog](https://ai.meta.com/blog/meta-llama-3-1/), so I wanted to use it as an example for distillation on this task, thus Llama 3.1 405B becomes the teacher model.\n",
    "\n",
    "The cheapest inference API i could find is via [Fireworks.ai](https://fireworks.ai/) which, at the time of making this, offer 1M Token `Input/Output` at `$3/$3` respectively. We'll use their integration with LangChain to instantiate."
   ]
  },
  {
   "cell_type": "code",
   "id": "28c3bf6e-ee39-4355-8129-f4ce0fdeb7c5",
   "metadata": {
    "id": "28c3bf6e-ee39-4355-8129-f4ce0fdeb7c5"
   },
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "AZURE_OPENAI_API_KEY=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "llm = AzureChatOpenAI (\n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    model=\"gpt-35-turbo\",\n",
    "    temperature=0.7\n",
    ")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T02:36:04.588105Z",
     "start_time": "2024-08-10T02:36:03.713330Z"
    },
    "id": "1348fdb7695ea506"
   },
   "cell_type": "code",
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(\n",
    "    api_key=GROQ_API_KEY,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    temperature=0.7\n",
    ")\n"
   ],
   "id": "1348fdb7695ea506",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "11adb6b5-3080-45ad-b3a6-bc2e591214d2",
   "metadata": {
    "id": "11adb6b5-3080-45ad-b3a6-bc2e591214d2"
   },
   "source": [
    "### Prompting\n",
    "\n",
    "To ensure we give our teacher model the best opportunity, we'll be employing two techniques in our classification prompt:\n",
    "1. **Chain-of-Thought Reasoning:** Making the language model write a reasoning description to \"think\" through the problem before giving an answer\n",
    "2. **Few-shot Prompting:** Providing robust examples about your expectations of both performance and format to better guide the LLM.\n",
    "\n",
    "Examples taken from entries within the rows 7101-7200, which will not be used during training or for testing further on"
   ]
  },
  {
   "cell_type": "code",
   "id": "99c93237-b1ba-41d6-a4a0-194d99c5f969",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T02:36:06.894949Z",
     "start_time": "2024-08-10T02:36:06.888274Z"
    },
    "id": "99c93237-b1ba-41d6-a4a0-194d99c5f969"
   },
   "source": [
    "tweet_sentiment_cot_prompt = \"\"\"\\\n",
    "You are a highly qualified expert trained to annotate machine learning training data.\n",
    "Your task is to briefly analyze the sentiment in the TEXT below from an social media manager perspective and then label it with only one the three labels:\n",
    "positive, negative, neutral.\n",
    "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about the context.\n",
    "You first reason step by step about the correct label and then return your label.\n",
    "You ALWAYS respond once in the following JSON format with brackets: {{\"reason\": \"...\", \"label\": \"...\"}}\n",
    "\n",
    "Examples:\n",
    "Text: Mode: Home Office\n",
    "JSON: {{\"reason\": \"The text is a factual statement about a work mode without expressing any emotion or opinion\", \"label\": \"neutral\"}}\n",
    "Text: oh oh oh are you offering to send ducks! I love love love confit duck\n",
    "JSON: {{\"reason\": \"The text expresses enthusiasm and love for confit duck, indicating a positive sentiment\", \"label\": \"positive\"}}\n",
    "Text: off to glue stuff onto poster\n",
    "JSON: {{\"reason\": \"The text is a simple statement of an action without any emotional context\", \"label\": \"neutral\"}}\n",
    "Text: Beautiful Day..takn it down twitters tell ALL mothers Happy Mothers Day\n",
    "JSON: {{\"reason\": \"The text describes a beautiful day and expresses positive wishes for Mother's Day\", \"label\": \"positive\"}}\n",
    "Text: Likewise. However, what was the comment about originally?\n",
    "JSON: {{\"reason\": \"The text is a neutral inquiry without expressing any particular sentiment\", \"label\": \"neutral\"}}\n",
    "Text: wished didnt spend money last night\n",
    "JSON: {{\"reason\": \"The text expresses regret about spending money, indicating a negative sentiment\", \"label\": \"negative\"}}\n",
    "Text: yo wake your **** up and go to work go get that paper u aint sick dont lie\n",
    "JSON: {{\"reason\": \"The text is aggressive and accusatory, suggesting a negative sentiment\", \"label\": \"negative\"}}\n",
    "Text: Such a beautiful morning\n",
    "JSON: {{\"reason\": \"The text expresses appreciation for the morning, indicating a positive sentiment\", \"label\": \"positive\"}}\n",
    "Text: Nooo...i forgot my calculator for physics oh well class is allmost over :3\n",
    "JSON: {{\"reason\": \"The text expresses initial disappointment about forgetting a calculator, indicating a negative sentiment\", \"label\": \"negative\"}}\n",
    "\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "21ee6aeb-dee8-48e8-889e-8dc5dabeefb7",
   "metadata": {
    "id": "21ee6aeb-dee8-48e8-889e-8dc5dabeefb7"
   },
   "source": [
    "and converting to an invokable chain via LangChain"
   ]
  },
  {
   "cell_type": "code",
   "id": "53896064-c5e8-423a-9160-40e38a80f22e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T02:36:08.687026Z",
     "start_time": "2024-08-10T02:36:08.670117Z"
    },
    "id": "53896064-c5e8-423a-9160-40e38a80f22e"
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", tweet_sentiment_cot_prompt\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \"Your TEXT to analyse: {text}\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | JsonOutputParser()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a61e1b25-a5f4-40c6-9f22-b0f62ce76d10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T02:36:11.199640Z",
     "start_time": "2024-08-10T02:36:10.490065Z"
    },
    "id": "a61e1b25-a5f4-40c6-9f22-b0f62ce76d10",
    "outputId": "9bbbf4f3-c894-4fdc-cd71-330c136049f0"
   },
   "source": [
    "# Example\n",
    "#response = chain.invoke(\"Want to get a Blackberry but can`t afford it. Just watching the telly and relaxing. Hard sesion tomorrow.\")\n",
    "response = chain.invoke(\"But i dont mind the long line when theres a super cutie in front of me. Too bad he`s wearing a **** bracelet with a girls name on it\")\n",
    "\n",
    "response"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6a10351a-8f6c-4fab-bdb3-6a5076632294",
   "metadata": {
    "id": "6a10351a-8f6c-4fab-bdb3-6a5076632294"
   },
   "source": [
    "### Annotation Script\n",
    "\n",
    "Now using our model + prompt to generate the annotations and combine them with the CSV. Error handling is generally due to the filtering that Llama 3.1 405B has to not generate answers when presented with innapropriate content.\n",
    "\n",
    "**NOTE:** This is roughly ~$10 of usage via API when ran over 5,000 examples. Run with caution! (when using fireworks API. I have pointed to Azure)"
   ]
  },
  {
   "cell_type": "code",
   "id": "da87583a-dbc5-452e-9d9d-c8769310c5f3",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-08-10T04:07:58.708417Z",
     "start_time": "2024-08-10T02:36:15.814790Z"
    },
    "id": "da87583a-dbc5-452e-9d9d-c8769310c5f3",
    "outputId": "faa4465e-300d-49c1-bcde-c9ee310193ec"
   },
   "source": [
    "import json, csv\n",
    "\n",
    "def process_csv(input_file, output_file):\n",
    "    i = 0\n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames + ['Llama_405B_reason', 'Llama_405B_label_text']\n",
    "\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            try:\n",
    "                # Invoke the chain with the text from the current row\n",
    "                response = chain.invoke({\"text\": row['text']})\n",
    "                result = json.loads(response) if isinstance(response, str) else response\n",
    "\n",
    "                # Add new fields to the row\n",
    "                row['Llama_405B_reason'] = result['reason']\n",
    "                row['Llama_405B_label_text'] = result['label']\n",
    "\n",
    "                # Write the updated row to the output file immediately\n",
    "                writer.writerow(row)\n",
    "\n",
    "                # Flush the write buffer to ensure data is written to disk\n",
    "                outfile.flush()\n",
    "\n",
    "                i+=1\n",
    "                print(f\"{i} - Processed and saved row with id: {row['id']}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Error handling\n",
    "                print(f\"Error processing row with id {row.get('id', 'unknown')}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Processing completed. Output saved to: {output_file}\")\n",
    "\n",
    "# Usage\n",
    "input_file = str(get_project_root())+\"/data/tweet_sentiment/twittersentiment_5001_6000_cleaned.csv\"\n",
    "output_file = str(get_project_root())+\"/data/tweet_sentiment/test.csv\"\n",
    "\n",
    "process_csv(input_file, output_file)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
