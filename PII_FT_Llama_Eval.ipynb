{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirb28/llm/blob/main/PII_FT_Llama_Eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAP3nI3kUUb0"
      },
      "source": [
        "# How to Supercharge Small Language Models\n",
        "\n",
        "Author: Thierry Moreau - Co-founder, Head of DevRel @ OctoAI\n",
        "\n",
        "Authored date: Aug 22nd 2024\n",
        "\n",
        "### Intro\n",
        "\n",
        "In this notebook you'll learn how to fine-tune an open source LLM (Llama3.1-8B) from scratch to perform a specialized task - PII redaction via function calling.\n",
        "\n",
        "**We'll show that taking a small and efficient LLM like Llama3.1-8B and fine-tuning it, you can achieve significant quality improvements over a state of the art model like GPT-4o, while also achieving significant cost savings (>32x cheaper).**\n",
        "\n",
        "![llm evaluation results](https://raw.githubusercontent.com/tmoreau89/image-assets/main/fine_tune/leaderboard-results.png)\n",
        "\n",
        "This notebook is divided into 4 parts:\n",
        "\n",
        "1. Fine-tuning dataset collection\n",
        "2. Kick off fine-tuning on OpenPipe\n",
        "3. Deploy your fine-tune on OctoAI\n",
        "4. Evaluate your fine-tune against GPT-4o under different prompting scenarios\n",
        "\n",
        "### Pre-requisites\n",
        "\n",
        "## OctoAI\n",
        "\n",
        "We'll use OctoAI to fine-tune our model, and deploy the resulting fine-tune on an inferencing endpoint.\n",
        "\n",
        "OctoAI offers efficient, customizable and reliable GenAI inference endpoints. You can sign up for an account on http://octoai.cloud/, and get access to the latest and greatest open source LLMs.\n",
        "\n",
        "Create an API token by following [this guide](https://octo.ai/docs/getting-started/how-to-create-an-octoai-access-token) and save it somewhere safe: we'll need it later in this notebook.\n",
        "\n",
        "By signing up you will automatically get $10 in credits, enough to generate 66.7M Llama3.1-8B tokens.\n",
        "\n",
        "## OpenPipe\n",
        "\n",
        "We'll use OpenPipe to fine tune our Llama3.1-8B model. OpenPipe lets developers build fine tuning datasets, fire off fine-tune jobs across a variety of base models, and run comprehensive quality evaluations.\n",
        "\n",
        "Get started by signing up for an account: https://openpipe.ai/.\n",
        "\n",
        "## Weights & Biases\n",
        "\n",
        "We'll use Weights & Biases to build an evaluation dashboard to compare the performance of the fine-tune against other models across different metrics (quality, performance, cost).\n",
        "\n",
        "Get started by signing up for an account: https://wandb.ai/site.\n",
        "\n",
        "## OpenAI (optional)\n",
        "\n",
        "We'll use OpenAI for quality comparisons against our fine-tuned LLM. More specifically we'll compare our fine-tune against GPT-4o and GPT-4o-mini.\n",
        "\n",
        "You can create an OpenAI account at the following URL: https://platform.openai.com. Create a new API key on [this link](https://platform.openai.com/api-keys) once you've created an account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k9AEzEZzMcD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Enter your OctoAI Token\n",
        "OCTOAI_TOKEN = getpass()\n",
        "os.environ[\"OCTOAI_TOKEN\"] = OCTOAI_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUt2H7CqzjWS"
      },
      "outputs": [],
      "source": [
        "# Enter your OpenAI Token\n",
        "OPENAI_API_KEY = getpass()\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the OctoAI CLI\n",
        "\n",
        "To deploy the OpenPipe Llama3.1-8B fine tune onto OctoAI inference endpoints, we'll need to use the OctoAI CLI."
      ],
      "metadata": {
        "id": "aleF87cM6WAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# install the octoai CLI\n",
        "!curl https://s3.amazonaws.com/downloads.octoai.cloud/octoai/install_octoai_cli_and_sdk.sh -sSfL | sh\n",
        "!apt-get install jq"
      ],
      "metadata": {
        "id": "r0Guaksa6XhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log into the OctoAI CLI using the $OCTOAI_TOKEN set in your environment\n",
        "!octoai login"
      ],
      "metadata": {
        "id": "tvNb3yrt6Y29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEcdrsjoYtCC"
      },
      "source": [
        "## Install Python Packages\n",
        "\n",
        "Run the cell below to install the necessary pip packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op-u7js3YxwU"
      },
      "outputs": [],
      "source": [
        "# Install and read in required packages\n",
        "print('⏳ Installing packages')\n",
        "%pip install -q openai datasets weave tqdm ipywidgets requests\n",
        "print('✅ Packages installed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu5uvz8PRkLl"
      },
      "source": [
        "## Log into Weave from W&B\n",
        "\n",
        "Run the cell below to authenticate yourself into weave with your W&B account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afhCyr10Rj5C"
      },
      "outputs": [],
      "source": [
        "import weave\n",
        "\n",
        "# Initialize W&B Weave project\n",
        "weave.init('llama31-vs-gpt4o-quality-exploration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoBISfX3or4r"
      },
      "source": [
        "# 1. Building a Fine-Tuning Dataset\n",
        "\n",
        "A fine-tuned model is only as good as the dataset it's been trained on. Therefore the dataset collection step is critical in order to get a high quality fine tune.\n",
        "\n",
        "We'll build our fine-tuning dataset by taking an already labeled dataset from HuggingFace and turning into a synthetic LLM request log, which we'll then upload to OctoAI in JSONL format.\n",
        "\n",
        "## PII Masking Dataset\n",
        "\n",
        "The dataset we're using in this notebook is the Personally Identifiable Information (PII) masking 200k dataset from [AI4Privacy](https://www.ai4privacy.com/), available via this [link on HuggingFace](https://huggingface.co/datasets/ai4privacy/pii-masking-200k).\n",
        "\n",
        "This dataset has about 200k synthetic text samples that each contain one or more PII entries across 54 PII classes.\n",
        "\n",
        "An example of PII redaction looks as follow.\n",
        "\n",
        "Input text:\n",
        "```\n",
        "Dear Omer, as per our records, your license 78B5R2MVFAHJ48500 is still registered in our records for access to the educational tools.\n",
        "```\n",
        "\n",
        "Redacted text:\n",
        "```\n",
        "Dear [FIRSTNAME], as per our records, your license [VEHICLEVIN] is still registered in our records for access to the educational tools.\n",
        "```\n",
        "\n",
        "Privacy mask:\n",
        "```\n",
        "[ { \"value\": \"Omer\", \"start\": 5, \"end\": 9, \"label\": \"FIRSTNAME\" }, { \"value\": \"78B5R2MVFAHJ48500\", \"start\": 44, \"end\": 61, \"label\": \"VEHICLEVIN\" } ]\n",
        "```\n",
        "\n",
        "## Function calling for PII redaction\n",
        "\n",
        "Instead of training an LLM to do the PII redaction directly on the input text, we'll use the LLM's tool calling ability to call a function that will perform the redaction on the original text instead.\n",
        "\n",
        "Using a function to perform the redaction gives us flexibility to implement different redaction approaches after the LLM has been fine tuned.\n",
        "* We can redact by replacing the PII with the PII class it belogs to, e.g. `Omer` becomes `[FIRSTNAME`]`.\n",
        "* We can redact by replacing the PII with masked information, e.g. `Omer` becomes `XXXXXX`.\n",
        "* We can redact by replacing the PII with a fake PII by mapping each unique original PII to a corresponding fake PII substitue from a database, e.g. `Omer` becomes `Kendall`.\n",
        "\n",
        "With the above system prompt and the function call definition, we'll invoke the LLM by passing in the text that needs to be redacted:\n",
        "\n",
        "```python\n",
        "\n",
        "import requests\n",
        "\n",
        "user_prompt = \"Dear Omer, as per our records, your license 78B5R2MVFAHJ48500 is still registered in our records for access to the educational tools.\"\n",
        "\n",
        "req = requests.post(\"https://text.octoai.run/v1/chat/completions\",\n",
        "    headers={\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {OCTOAI_TOKEN}\"\n",
        "    },\n",
        "    json={\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}]\n",
        "        \"model\": \"openpipe-llama-3-8b-32k\",\n",
        "        \"max_tokens\": 512,\n",
        "        \"presence_penalty\": 0,\n",
        "        \"temperature\": 0,\n",
        "        \"top_p\": 0.9,\n",
        "        \"peft\": lora_asset_name, # we'll talk about what to set this to next\n",
        "        \"tool_choice\": tool_choice, # defined in the next cell\n",
        "        \"tools\": tools # defined in the next cell\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response.json()[\"choices\"][0][\"message\"][\"tool_calls\"][0][\"function\"][\"arguments\"])\n",
        "```\n",
        "\n",
        "Which when fine-tuned correcty, should return the following chat completions response containing the  arguments to pass into the `redact()` function call:\n",
        "```json\n",
        "{\n",
        "  'fields_to_redact':\n",
        "  [\n",
        "    {\n",
        "      'string': 'Omer',\n",
        "      'pii_type': 'FIRSTNAME'\n",
        "    },\n",
        "    {\n",
        "      'string': '78B5R2MVFAHJ48500',\n",
        "      'pii_type': 'VEHICLEVIN'\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "With that, let's recap how the LLM will be invoked to perform PII redaction via function calling:\n",
        "\n",
        "![llm deployment cycle](https://raw.githubusercontent.com/tmoreau89/image-assets/main/fine_tune/llm_call_overview.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_-q4Yw-l0jJ"
      },
      "outputs": [],
      "source": [
        "# Define the system prompt with all of the PII categories\n",
        "# The nice thing about this system prompt is that it can be very easily extended\n",
        "# to fit your unique use case.\n",
        "piir_system_prompt = \"\"\"\n",
        "You are an expert model trained to redact potentially sensitive information from documents. You have been given a document to redact. Your goal is to accurately redact the sensitive information from the document. Sensitive information can be in one of the following categories:\n",
        "\n",
        "- ACCOUNTNAME: name of an account\n",
        "- ACCOUNTNUMBER: number of an account\n",
        "- AGE: a person's age\n",
        "- AMOUNT: information indicating a certain monetary amount\n",
        "- BIC: a business identifier code\n",
        "- BITCOINADDRESS: bitcoint address, generally stored in a cryptocurrency wallet\n",
        "- BUILDINGNUMBER: number of a building in a physical address\n",
        "- CITY: name of a city indicating location or address\n",
        "- COMPANYNAME: name of a company\n",
        "- COUNTRY: name of a country indicating location or address\n",
        "- CREDITCARDCVV: credit card CVV\n",
        "- CREDITCARDISSUER: credit card issuer\n",
        "- CREDITCARDNUMBER: credit card number\n",
        "- CURRENCY: currency of a balance or transaction\n",
        "- CURRENCYCODE: the code a currency (e.g. USD)\n",
        "- CURRENCYNAME: name of a currency (e.g. US dollar)\n",
        "- CURRENCYSYMBOL: symbol of a currency (e.g. $)\n",
        "- DATE: a specific calendar date\n",
        "- DOB: a specific calendar date representing birth\n",
        "- EMAIL: an email ID\n",
        "- ETHEREUMADDRESS: ethereum address, generally stored in a cryptocurrency wallet\n",
        "- EYECOLOR: eye color, used to identify a person\n",
        "- FIRSTNAME: first name of a person\n",
        "- GENDER: a gender identifier\n",
        "- HEIGHT: height of a person\n",
        "- IBAN: international banking account number\n",
        "- IP: IP address\n",
        "- IPV4: IP v4 address\n",
        "- IPV6: IP v6 address\n",
        "- JOBAREA: job area, specialization or category\n",
        "- JOBTITLE: job title\n",
        "- LASTNAME: last name of a person\n",
        "- LITECOINADDRESS: litecoin address, generally stored in a cryptocurrency wallet\n",
        "- MAC: MAC address\n",
        "- MASKEDNUMBER: masked number\n",
        "- MIDDLENAME: middle name of a person\n",
        "- NEARBYGPSCOORDINATE: nearby GPS coordinates\n",
        "- ORDINALDIRECTION: ordinal direction (north, south, northeast, etc.)\n",
        "- PASSWORD: a secure string used for authentication\n",
        "- PHONEIMEI: the IMEI of a phone\n",
        "- PHONENUMBER: a telephone number\n",
        "- PIN: a personal identificaiton number (PIN)\n",
        "- PREFIX: prefix used to identify a person (Mr., Mrs., Dr. etc.)\n",
        "- SECONDARY ADDRESS: a secondary physical address address\n",
        "- SEX: a sex identifier (male/female)\n",
        "- SSN: a social security number\n",
        "- STATE: name of a state indicating location or address\n",
        "- STREET: name of a street indicating location or address\n",
        "- TIME: time of the day\n",
        "- URL: URL of a website\n",
        "- USERAGENT: user agent to identify the application, operating system, vendor etc.\n",
        "- USERNAME: user name to identify user\n",
        "- VERHICLEVIN: vehicle identification number or license number\n",
        "- VEHICLEVRM: vehicle registration mark\n",
        "- ZIPCODE: zipcode indicating location or address\n",
        "\n",
        "You should return the specific string that needs to be redacted, along with the category of sensitive information that it belongs to. If there is no sensitive information in the document, return no strings.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpo7VMozl-3V"
      },
      "outputs": [],
      "source": [
        "# Define the tools for the LLM to invoke\n",
        "\n",
        "piir_tool_choice = {\n",
        "  \"type\": \"function\",\n",
        "  \"function\": {\"name\": \"redact\"}\n",
        "}\n",
        "\n",
        "piir_tools = [\n",
        "  {\n",
        "    \"function\": {\n",
        "      \"name\": \"redact\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"fields_to_redact\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "              \"type\": \"object\",\n",
        "              \"required\": [\n",
        "                \"string\",\n",
        "                \"pii_type\"\n",
        "              ],\n",
        "              \"properties\": {\n",
        "                \"string\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"The exact matching string to redact. Include any whitespace or punctuation. Must be an exact string match!\"\n",
        "                },\n",
        "                \"pii_type\": {\n",
        "                  \"enum\": [\n",
        "                    \"ACCOUNTNAME\",\n",
        "                    \"ACCOUNTNUMBER\",\n",
        "                    \"AGE\",\n",
        "                    \"AMOUNT\",\n",
        "                    \"BIC\",\n",
        "                    \"BITCOINADDRESS\",\n",
        "                    \"BUILDINGNUMBER\",\n",
        "                    \"CITY\",\n",
        "                    \"COMPANYNAME\",\n",
        "                    \"COUNTY\",\n",
        "                    \"CREDITCARDCVV\",\n",
        "                    \"CREDITCARDISSUER\",\n",
        "                    \"CREDITCARDNUMBER\",\n",
        "                    \"CURRENCY\",\n",
        "                    \"CURRENCYCODE\",\n",
        "                    \"CURRENCYNAME\",\n",
        "                    \"CURRENCYSYMBOL\",\n",
        "                    \"DATE\",\n",
        "                    \"DOB\",\n",
        "                    \"EMAIL\",\n",
        "                    \"ETHEREUMADDRESS\",\n",
        "                    \"EYECOLOR\",\n",
        "                    \"FIRSTNAME\",\n",
        "                    \"GENDER\",\n",
        "                    \"HEIGHT\",\n",
        "                    \"IBAN\",\n",
        "                    \"IP\",\n",
        "                    \"IPV4\",\n",
        "                    \"IPV6\",\n",
        "                    \"JOBAREA\",\n",
        "                    \"JOBTITLE\",\n",
        "                    \"JOBTYPE\",\n",
        "                    \"LASTNAME\",\n",
        "                    \"LITECOINADDRESS\",\n",
        "                    \"MAC\",\n",
        "                    \"MASKEDNUMBER\",\n",
        "                    \"MIDDLENAME\",\n",
        "                    \"NEARBYGPSCOORDINATE\",\n",
        "                    \"ORDINALDIRECTION\",\n",
        "                    \"PASSWORD\",\n",
        "                    \"PHONEIMEI\",\n",
        "                    \"PHONENUMBER\",\n",
        "                    \"PIN\",\n",
        "                    \"PREFIX\",\n",
        "                    \"SECONDARYADDRESS\",\n",
        "                    \"SEX\",\n",
        "                    \"SSN\",\n",
        "                    \"STATE\",\n",
        "                    \"STREET\",\n",
        "                    \"TIME\",\n",
        "                    \"URL\",\n",
        "                    \"USERAGENT\",\n",
        "                    \"USERNAME\",\n",
        "                    \"VEHICLEVIN\",\n",
        "                    \"VEHICLEVRM\",\n",
        "                    \"ZIPCODE\"\n",
        "                  ],\n",
        "                  \"type\": \"string\"\n",
        "                }\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    \"type\": \"function\"\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32n1tctTnBg4"
      },
      "source": [
        "## Preparing the dataset\n",
        "\n",
        "In the next cell we'll load the dataset from Huggingface and generate and build a 10k sample large fine-tuning dataset using a combination of the input text as LLM prompt, and a re-worked privacy mask as the expected tools call response from the LLM.\n",
        "\n",
        "Since there are 200k samples and we're only using 10k for our fine-tuning dataset, we have more than enough samples remaining as a holdout set to run post-training evaluations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wsr4U_7LtFdc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from huggingface\n",
        "dataset = load_dataset(\"ai4privacy/pii-masking-200k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1Mrqq9ss5Rt"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "# Fine-tuning dataset size\n",
        "# To improve on accuracy results you can bump the size to a larger value\n",
        "TRAINING_SIZE = 10000\n",
        "\n",
        "# Create a dataset for fine tuning\n",
        "training_dataset = []\n",
        "for idx, item in enumerate(dataset['train'].select(range(0, TRAINING_SIZE))):\n",
        "    function_arguments = {\n",
        "        \"fields_to_redact\": []\n",
        "    }\n",
        "    for i in item[\"privacy_mask\"]:\n",
        "        function_arguments[\"fields_to_redact\"].append({\n",
        "            \"string\": i[\"value\"],\n",
        "            \"pii_type\": i[\"label\"]\n",
        "        })\n",
        "    dataitem = {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": piir_system_prompt\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": item[\"source_text\"]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": None,\n",
        "                \"tool_calls\":\n",
        "                    [\n",
        "                        {\n",
        "                            \"id\":\"\",\n",
        "                            \"type\":\"function\",\n",
        "                            \"function\":\n",
        "                            {\n",
        "                                \"name\": \"redact\",\n",
        "                                \"arguments\": json.dumps(function_arguments, indent=2)\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "            },\n",
        "        ],\n",
        "        \"tools\": piir_tools,\n",
        "        \"tool_choice\": piir_tool_choice\n",
        "    }\n",
        "    training_dataset.append(dataitem)\n",
        "\n",
        "file_path='training_dataset.jsonl'\n",
        "with open(file_path, 'w') as outfile:\n",
        "    for entry in training_dataset:\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')\n",
        "\n",
        "# This will let you download the file from your browser in case you'd like to view\n",
        "files.download(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Fine tune the LLM\n",
        "\n",
        "We'll use OpenPipe for this step.\n",
        "\n",
        "## Upload the dataset to OpenPipe\n",
        "\n",
        "Check your downloads folder, you should find an `openpipe_dataset.jsonl` in there.\n",
        "\n",
        "Now follow along the instructions on [this page](https://docs.openpipe.ai/features/exporting-data#dataset-export) to upload your dataset on OpenPipe.\n",
        "\n",
        "1. On your OpenPipe console, click on \"Datasets\" listed in the bar on the left.\n",
        "2. Click on \"+ New Dataset\" button at the top right of the window.\n",
        "3. Click on \"Upload Data\" button at the top left of the window.\n",
        "4. Drop the jsonl file that was just downloaded in the \"Upload File\" window.\n",
        "5. Click on the Upload button and wait for the dataset to get uploaded.\n",
        "\n",
        "You'll get the confirmation window below if the dataset gets successfully uploaded.\n",
        "\n",
        "![upload confirmation window](https://raw.githubusercontent.com/tmoreau89/image-assets/main/fine_tune/openpipe_dataset_uploaded.png)\n",
        "\n",
        "\n",
        "You'll see your dataset entries under the \"Dataset view\" - 10,000 of them which should have gotten split into a 9,000 training and 1,000 test set.\n",
        "\n",
        "![dataset view](https://raw.githubusercontent.com/tmoreau89/image-assets/main/fine_tune/dataset_view.png)\n",
        "\n",
        "Hit \"Settings\", and rename your Dataset if need be.\n",
        "\n",
        "## Launch a fine-tune\n",
        "\n",
        "Under the Dataset view on the OpenPipe console, you can launch a fine tune by clicking on the \"Fine Tune\" button at the top right of the window.\n",
        "\n",
        "You can define a model ID - this lets us uniquely identify the resulting fine tuned model.\n",
        "\n",
        "Next you can chose your base model:\n",
        "* You can choose between open source models (Llama, Mistral) or closed source models (OpenAI GPT). Selecting an open source model gives you ownership of the weights, and lets you deploy the model on the platform of your choice. For this notebook we'll select the \"Llama 3.1 8B\" model.\n",
        "* You'll see that we have a good working set size to work with with 10k training samples.\n",
        "* Finally you can choose to tweak the advanced options but we'll leave them as-is.\n",
        "\n",
        "![fine tuning settings](https://raw.githubusercontent.com/tmoreau89/image-assets/main/fine_tune/finetuning_launch_llama31.png)\n",
        "\n",
        "Let's go ahead and hit \"Start Training\" to kick off the fine-tuning job."
      ],
      "metadata": {
        "id": "gB2WHQKA7Ov-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 3. Deploy the fine-tuned LLM\n",
        "\n",
        "In this section we'll export the model weights in LoRA FP16 form to be hosted on OctoAI.\n",
        "\n",
        "You'll be informed that the fine tune job has completed by email. Once you've been notified you can proceed with the steps below.\n",
        "\n",
        "## Export your model weights\n",
        "\n",
        "Access your fine-tuned model by clicking on \"Fine Tune\" on the OpenPipe console.\n",
        "Click on the model that was just fine tuned.\n",
        "\n",
        "![model fine tune](https://raw.githubusercontent.com/tmoreau89/image-assets/main/fine_tune/model_finetune.png)\n",
        "\n",
        "At the bottom of the page, you can select a format that the model weights gets exported in. Select \"LoRA:FP32\" under the \"Format\" drop-down and hit Export Weights.\n",
        "\n",
        "It will take a couple of minutes until your model weights are ready for download. When the weights are ready, right click on \"Download Weights\" to copy the link to the weights and set the URL aside, which we'll need in the next step to set `lora_url`.\n",
        "\n",
        "![fine tune download link](https://raw.githubusercontent.com/tmoreau89/image-assets/main/fine_tune/model_finetune_link.png)\n",
        "\n",
        "Below, you'll need to set the `lora_url` to the URL you just copied from the \"Download Weights\" link in the previous step.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Byhl03UP7UWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Llama 3.1 instruct\n",
        "checkpoint_name = \"octoai:meta-llama-3-1-8b-instruct\"\n",
        "lora_url = \"SET ME\"\n",
        "assert(lora_url != \"SET ME\") # Please update lora_url\n",
        "\n",
        "# Define an asset name on OctoAI to uniquely identify the LoRA\n",
        "lora_asset_name = \"pii-redaction-finetune-{}\".format(str(random.randint(1000, 999999)))\n",
        "\n",
        "# Set checkpoint name and LoRA URL as env vars\n",
        "%env checkpoint_name=$checkpoint_name\n",
        "%env lora_url=$lora_url\n",
        "%env lora_asset_name=$lora_asset_name"
      ],
      "metadata": {
        "id": "-HaWRFJO7buG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we upload the LoRA to OctoAI.\n",
        "\n",
        "We need to specify what base checkpoint and architecture (\"engine\") the model corresponds to.\n",
        "\n",
        "The command below uses \"--upload-from-url\" which lets you upload these files from the OpenPipe download URL. Note also that there is an \"upload-from-dir\" that lets you specify a local directory if you've downloaded the LoRA zip file on your local drive.\n",
        "\n",
        "The \"--wait\" flag allows to block until the upload has completed, making scripting possible."
      ],
      "metadata": {
        "id": "Y4tHDAYl7rHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "octoai asset create \\\n",
        "  --checkpoint $checkpoint_name \\\n",
        "  --format safetensors \\\n",
        "  --type lora \\\n",
        "  --engine text/llama-3.1-8b \\\n",
        "  --name $lora_asset_name \\\n",
        "  --data-type fp16 \\\n",
        "  --upload-from-url $lora_url \\\n",
        "  --wait"
      ],
      "metadata": {
        "id": "1obK-JLg7qVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can double check to make sure the LoRA got added using `octoai asset list` command."
      ],
      "metadata": {
        "id": "7OXO2T547vGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!octoai asset list"
      ],
      "metadata": {
        "id": "gg8CETor7wbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTsL8swioFKI"
      },
      "source": [
        "## Sanity checking that the fine-tune is running on OctoAI\n",
        "\n",
        "Let's go ahead and use OctoAI to run a test inference with the code below. It's doable by supplying the LoRA name as `peft` parameter (a LoRA is a type of Parameter-Efficient Fine Tune) when making a call to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUT1waisoXFs"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "print(\"Using assset \", lora_asset_name)\n",
        "\n",
        "test_prompt = \"Dear Omer, as per our records, your license 78B5R2MVFAHJ48500 is still registered in our records for access to the educational tools. Please feedback on it's operability.\"\n",
        "\n",
        "messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": piir_system_prompt\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": test_prompt\n",
        "    },\n",
        "]\n",
        "\n",
        "req = requests.post(\"https://text.octoai.run/v1/chat/completions\",\n",
        "    headers={\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": \"Bearer {}\".format(os.environ[\"OCTOAI_TOKEN\"])\n",
        "    },\n",
        "    json={\n",
        "        \"messages\": messages,\n",
        "        \"model\": \"meta-llama-3.1-8b-instruct-lora\",\n",
        "        \"max_tokens\": 512,\n",
        "        \"presence_penalty\": 0,\n",
        "        \"temperature\": 0,\n",
        "        \"top_p\": 0.9,\n",
        "        \"peft\": lora_asset_name,\n",
        "        \"tool_choice\": piir_tool_choice,\n",
        "        \"tools\": piir_tools\n",
        "    }\n",
        ")\n",
        "\n",
        "print(json.dumps(req.json(),indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiMkaUJLqGzp"
      },
      "source": [
        "The output looks good! Let's now run a more exhaustive set of quality evaluations to see where this model stands next to very capable LLMs like GPT4o\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp8Lp4GIkTqb"
      },
      "source": [
        "# 4. Quality Evaluations Using W&B Weave\n",
        "\n",
        "## Model Coverage\n",
        "\n",
        "In this section we'll run quality tests between:\n",
        "* Our Llama3.1-8b model fine tune hosted by OctoAI\n",
        "* A Llama3.1-8b-instruct model hosted by OctoAI (not tuned for the task)\n",
        "* A GPT4o model hosted by OpenAI\n",
        "\n",
        "# Techniques Coverage\n",
        "We evaluate different prompting techniques for the non-tuned Llama3.1-8b and GPT4o models, namely:\n",
        "* Zero-shot prompting\n",
        "* Single-shot prompting\n",
        "* Few-shot prompting\n",
        "\n",
        "With zero-shot prompting, we invoke the language model by passing in a system prompt and a user prompt. This is the most basic way to invoke the model’s chat completions API.\n",
        "\n",
        "With single-shot prompting, we insert in the messages a full conversation round trip that includes an example user prompt (example text to be redacted), the agent’s first response (language model determining what tool to call and what arguments to pass in the tool call, i.e. PII to redact), the tool response (containing the correctly redacted text scrubbed from its PII), and the agent’s second response (language model providing the correctly redacted text back to the user).\n",
        "\n",
        "With few-shot prompting, we insert more than one full conversation round trip to supplement additional usage examples.\n",
        "\n",
        "\n",
        "![few-shot prompting](https://raw.githubusercontent.com/tmoreau89/image-assets/main/fine_tune/few_shot_prompting.png)\n",
        "\n",
        "## Metrics\n",
        "\n",
        "We'll use Weights & Biases Weave library to monitor various metrics across our LLMs:\n",
        "* `has_response` as a measure of LLM robustness/availability overall\n",
        "* `has_tools_call` as a measure of LLM's ability to properly handle tools call requests\n",
        "* `cost_cents` as a measure of how expensive the LLM inference is\n",
        "* `quality_score` as a measure of how high the LLM quality is at the PII redaction task\n",
        "* `model_latency` as a measure of performance/speed\n",
        "\n",
        "## Quality Evaluation\n",
        "\n",
        "All quality evaluations start by defining a quality metric. In our case, we already have a labeled dataset from AI4Privacy, which we can use as our ground evaluation ground truth.\n",
        "\n",
        "We introduce a scoring system that works fairly simply. Each PII that needs to be redacted is represented as a pair containing:\n",
        "* The PII string itself, e.g. `5943919109159496`\n",
        "* The PII class, e.g. `CREDITCARDNUMBER`\n",
        "\n",
        "We use the SequenceMatcher library to obtain a similarity score between the ground truth PII and the one that's been inferred by the LLMs.\n",
        "\n",
        "If the PII string and class match perfectly, we get a score of 1.0. If any information starts to divert (e.g. LLM classifies PII as `MIDDLENAME` instead of `FIRSTNAME`, the score becomes lower, but is not 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC0QUHwPlJhZ"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import copy\n",
        "\n",
        "# PII redaction function\n",
        "def redact(text: str, fields_to_redact: list[dict]) -> str:\n",
        "    for elem in fields_to_redact:\n",
        "        text = text.replace(elem[\"string\"], \"[{}]\".format(elem[\"pii_type\"]))\n",
        "    return text\n",
        "\n",
        "# Generates few-shot prompts in messages payload\n",
        "def get_few_shot_prompts(dataset, n=1):\n",
        "    assert n > 0\n",
        "    messages = [\n",
        "      dataset[0]['messages'][0],\n",
        "    ]\n",
        "    for idx in range(0, n):\n",
        "        messages.append(dataset[idx]['messages'][1]) # user message\n",
        "        messages.append(dataset[idx]['messages'][2]) # tools response\n",
        "        # Get tools response\n",
        "        f_value = redact(\n",
        "            text=dataset[idx]['messages'][1]['content'],\n",
        "            fields_to_redact=json.loads(dataset[idx]['messages'][2]['tool_calls'][0]['function']['arguments'])['fields_to_redact']\n",
        "        )\n",
        "        messages.append(\n",
        "            {\n",
        "                \"tool_call_id\": \"\",\n",
        "                \"role\": \"tool\",\n",
        "                \"name\": \"redact\",\n",
        "                \"content\": f_value\n",
        "            }\n",
        "        )\n",
        "        messages.append({\"role\": \"assistant\", \"content\": f_value})\n",
        "\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB8R_3cEllf7"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Temperature\n",
        "# Set to 0 for reproducibility\n",
        "temperature = 0\n",
        "\n",
        "# OctoAI max tokens\n",
        "octoai_max_tokens = 32768\n",
        "\n",
        "# Cost Table for LLMs from OctoAI/OpenAI\n",
        "# As of Aug 22nd 2024\n",
        "# Everything is prices in $/M token\n",
        "cost_table = {\n",
        "    lora_asset_name: {\n",
        "        \"input\": 0.15,\n",
        "        \"output\": 0.15\n",
        "    },\n",
        "    \"meta-llama-3.1-8b-instruct\": {\n",
        "        \"input\": 0.15,\n",
        "        \"output\": 0.15\n",
        "    },\n",
        "    \"gpt-4o-2024-08-06\": {\n",
        "        \"input\": 2.5,\n",
        "        \"output\": 10\n",
        "    }\n",
        "}\n",
        "\n",
        "def chat_completions_infer(client, model: str, data: dict, shot_prompting: int) -> dict:\n",
        "    system_msg = data['messages'][0]\n",
        "    system_msg_content = system_msg['content']\n",
        "    user_msg = data['messages'][1]\n",
        "    user_msg_content = user_msg['content']\n",
        "\n",
        "    # Pre-process - multi-shot\n",
        "    if shot_prompting > 0:\n",
        "        messages = get_few_shot_prompts(\n",
        "            # sorted_training_dataset,\n",
        "            training_dataset,\n",
        "            n=shot_prompting\n",
        "        )\n",
        "        messages.append(user_msg)\n",
        "        data['messages'] = messages\n",
        "\n",
        "    # Invoke LLM\n",
        "    try:\n",
        "        response = client.chat.completions.create(**data)\n",
        "        response = json.loads(response.model_dump_json())\n",
        "    except:\n",
        "        return None\n",
        "    # Process outputs\n",
        "    output = None\n",
        "    usage = None\n",
        "    try:\n",
        "      tool_calls = response['choices'][0]['message']['tool_calls']\n",
        "      output = json.loads(tool_calls[0]['function']['arguments'])\n",
        "      usage = response['usage']\n",
        "    except:\n",
        "      pass\n",
        "    # Fix for OctoAI Llama3.1 models\n",
        "    try:\n",
        "        output[\"fields_to_redact\"] = json.loads(output[\"fields_to_redact\"])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return {\n",
        "        \"model_name\": model,\n",
        "        \"output\": output,\n",
        "        \"usage\": usage\n",
        "    }\n",
        "\n",
        "\n",
        "class OctoAILLM_FT(weave.Model):\n",
        "    model: str\n",
        "    shot_prompting: int\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, system_prompt: str, user_prompt: str, tool_choice: str, tools: str) -> dict:\n",
        "        client = OpenAI(\n",
        "            base_url=\"https://text.octoai.run/v1\",\n",
        "            api_key=os.environ[\"OCTOAI_TOKEN\"],\n",
        "        )\n",
        "        data = {\n",
        "            \"model\": \"meta-llama-3.1-8b-instruct-lora\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": octoai_max_tokens,\n",
        "            \"tool_choice\": tool_choice,\n",
        "            \"tools\": tools,\n",
        "            \"extra_body\": {\"peft\": self.model}\n",
        "        }\n",
        "\n",
        "        return chat_completions_infer(client, self.model, data, self.shot_prompting)\n",
        "\n",
        "\n",
        "class OctoAILLM(weave.Model):\n",
        "    model: str\n",
        "    shot_prompting: int\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, system_prompt: str, user_prompt: str, tool_choice: str, tools: str) -> dict:\n",
        "        client = OpenAI(\n",
        "            base_url=\"https://text.octoai.run/v1\",\n",
        "            api_key=os.environ[\"OCTOAI_TOKEN\"],\n",
        "        )\n",
        "        data = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": octoai_max_tokens,\n",
        "            \"tool_choice\": tool_choice,\n",
        "            \"tools\": tools\n",
        "        }\n",
        "        return chat_completions_infer(client, self.model, data, self.shot_prompting)\n",
        "\n",
        "\n",
        "class OpenAILLM(weave.Model):\n",
        "    model: str\n",
        "    shot_prompting: int\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, system_prompt: str, user_prompt: str, tool_choice: str, tools: str) -> dict:\n",
        "        client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "        data = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            \"temperature\": temperature,\n",
        "            \"tool_choice\": tool_choice,\n",
        "            \"tools\": tools\n",
        "        }\n",
        "        return chat_completions_infer(client, self.model, data, self.shot_prompting)\n",
        "\n",
        "\n",
        "# Llama3.1-8B-FT\n",
        "octoai_llama31_8b_ft = OctoAILLM_FT(model=lora_asset_name, shot_prompting=0)\n",
        "# Llama3.1-8B\n",
        "octoai_llama31_8b = OctoAILLM(model='meta-llama-3.1-8b-instruct', shot_prompting=0)\n",
        "octoai_llama31_8b_ss = OctoAILLM(model='meta-llama-3.1-8b-instruct', shot_prompting=1)\n",
        "octoai_llama31_8b_ms = OctoAILLM(model='meta-llama-3.1-8b-instruct', shot_prompting=2)\n",
        "# GPT-4o\n",
        "openai_gpt4o = OpenAILLM(model='gpt-4o-2024-08-06', shot_prompting=0)\n",
        "openai_gpt4o_ss = OpenAILLM(model='gpt-4o-2024-08-06', shot_prompting=1)\n",
        "openai_gpt4o_ms = OpenAILLM(model='gpt-4o-2024-08-06', shot_prompting=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdwkZxgSl-h2"
      },
      "outputs": [],
      "source": [
        "# Let's build a weave dataset out of holdout data from our original\n",
        "# huggingface dataset\n",
        "from weave import Dataset\n",
        "\n",
        "# Set to a larger number but as a test start,run with 10 to\n",
        "# sanity check that all is working as intended\n",
        "TEST_SIZE = 10\n",
        "\n",
        "pii_redaction_test_data = Dataset(\n",
        "    name=\"pii_redaction_data\",\n",
        "    rows=[\n",
        "        {\n",
        "            \"source_text\": t[\"source_text\"],\n",
        "            \"fields_to_redact\": [\n",
        "                {\n",
        "                    \"string\": elem[\"value\"],\n",
        "                    \"pii_type\": elem[\"label\"]\n",
        "                } for elem in t[\"privacy_mask\"]\n",
        "            ],\n",
        "        } for t in dataset['train'].select(range(TRAINING_SIZE, TRAINING_SIZE+TEST_SIZE))\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roE4cQLv-1I3"
      },
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "# All of the instantiated models\n",
        "models = [\n",
        "    octoai_llama31_8b_ft,\n",
        "    octoai_llama31_8b,\n",
        "    octoai_llama31_8b_ss,\n",
        "    octoai_llama31_8b_ms,\n",
        "    openai_gpt4o,\n",
        "    openai_gpt4o_ss,\n",
        "    openai_gpt4o_ms\n",
        "]\n",
        "\n",
        "# Similarity score helper function for assessing quality\n",
        "def similar(a, b):\n",
        "    a_string = \"{}, {}\".format(a['string'], a['pii_type'])\n",
        "    b_string = \"{}, {}\".format(b['string'], b['pii_type'])\n",
        "    return SequenceMatcher(None, a_string, b_string).ratio()\n",
        "\n",
        "# Define our scoring functions\n",
        "@weave.op()\n",
        "def has_response(model_output: dict) -> dict:\n",
        "    return {'has_response': model_output is not None}\n",
        "\n",
        "@weave.op()\n",
        "def has_tools_call(model_output: dict) -> dict:\n",
        "    try:\n",
        "        return {'has_tools_call': model_output['output'] is not None}\n",
        "    except:\n",
        "        return {'has_tools_call': False}\n",
        "\n",
        "@weave.op()\n",
        "def is_expensive(model_output: dict) -> dict:\n",
        "    try:\n",
        "        usage = model_output['usage']\n",
        "        model = model_output['model_name']\n",
        "        cost = int(usage['prompt_tokens']) * cost_table[model]['input'] + \\\n",
        "              int(usage['completion_tokens']) * cost_table[model]['output']\n",
        "        cost = cost / 1000000 * 100\n",
        "        return {'cost_cents': cost}\n",
        "    except:\n",
        "        return {'cost_cents': None}\n",
        "\n",
        "@weave.op()\n",
        "def is_good_quality(fields_to_redact: dict, model_output: dict) -> dict:\n",
        "    try:\n",
        "        # Assess accuracy\n",
        "        score = 0\n",
        "        # sum of all of the best similarity scores across the test PII\n",
        "        for t in model_output['output']['fields_to_redact']:\n",
        "            # we retain the best similarity score across all pairwise PII comparisons\n",
        "            best_score = 0\n",
        "            for r in fields_to_redact:\n",
        "                sim_score = similar(r, t)\n",
        "                if sim_score > best_score:\n",
        "                    best_score = sim_score\n",
        "            score += best_score\n",
        "        # divide the sum by the max of PII classes in reference data, and test data\n",
        "        # this is a simple formula to introduce a penalty in case we have a false positive or false negative\n",
        "        score = score/max(len(fields_to_redact), len(model_output['output']['fields_to_redact']))\n",
        "        return {'quality_score': score}\n",
        "    except:\n",
        "        return {'quality_score': 0}\n",
        "\n",
        "# Define the preprocess_model_input function\n",
        "def preprocess_model_input(row):\n",
        "    return {\n",
        "        'system_prompt': piir_system_prompt,\n",
        "        'user_prompt': row['source_text'],\n",
        "        'tool_choice': piir_tool_choice,\n",
        "        'tools': piir_tools\n",
        "    }\n",
        "\n",
        "# Define the evaluation\n",
        "ner_evaluation = weave.Evaluation(\n",
        "    name='pii_redaction_eval',\n",
        "    dataset=pii_redaction_test_data,\n",
        "    trials=1,\n",
        "    scorers=[\n",
        "        has_response,\n",
        "        has_tools_call,\n",
        "        is_expensive,\n",
        "        is_good_quality\n",
        "    ],\n",
        "    preprocess_model_input=preprocess_model_input\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdRpGOxa_nBI"
      },
      "outputs": [],
      "source": [
        "# Run evaluation for each model\n",
        "# Set to 2 will run the eval slowly, but you won't run into RPM limits\n",
        "# with any of the providers\n",
        "os.environ[\"WEAVE_PARALLELISM\"] = \"2\"\n",
        "\n",
        "pii_results = {}\n",
        "for model in models:\n",
        "    print(f\"Evaluating {model.model}...\")\n",
        "    result = await ner_evaluation.evaluate(model)\n",
        "    pii_results[model.name] = result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}